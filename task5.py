{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM5QoZ7AejUEYfIeDfwoGxC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kanimozhi-Perumal/Kanimozhi-Perumal/blob/main/task5.py\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h3NjyQyGRBXY"
      },
      "outputs": [],
      "source": [
        "#Neural style transfer is a technique that uses deep learning to apply the artistic style of one image (the style image) to the content of another image (the content image). This process involves leveraging pre-trained convolutional neural networks (CNNs) to extract features from both images and then optimizing the content image to minimize differences with the style image in terms of style and content features.\n",
        "\n",
        "#Hereâ€™s how you can implement neural style transfer using TensorFlow and Keras:\n",
        "\n",
        "#Steps to Implement Neural Style Transfer\n",
        "#Step 1: Import Libraries and Load Pre-trained Model\n",
        "#First, import necessary libraries and load a pre-trained CNN model. We'll use the VGG19 model, which is commonly used in style transfer due to its good balance between computational efficiency and performance.\n",
        "\n",
        "#python code\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.applications import vgg19\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "\n",
        "# Load VGG19 model pretrained on ImageNet\n",
        "model = vgg19.VGG19(weights='imagenet', include_top=False)\n",
        "#Step 2: Define Functions for Preprocessing and Postprocessing Images\n",
        "#Define functions to preprocess images for the VGG19 model and to postprocess the generated image after style transfer.\n",
        "\n",
        "\n",
        "\n",
        "def preprocess_image(image_path):\n",
        "    img = load_img(image_path, target_size=(224, 224))\n",
        "    img = img_to_array(img)\n",
        "    img = np.expand_dims(img, axis=0)\n",
        "    img = vgg19.preprocess_input(img)\n",
        "    return img\n",
        "\n",
        "def deprocess_image(img):\n",
        "    img = img.reshape((img.shape[1], img.shape[2], 3))\n",
        "    # Remove zero-center by mean pixel\n",
        "    img[:, :, 0] += 103.939\n",
        "    img[:, :, 1] += 116.779\n",
        "    img[:, :, 2] += 123.68\n",
        "    # 'BGR' to 'RGB'\n",
        "    img = img[:, :, ::-1]\n",
        "    img = np.clip(img, 0, 255).astype('uint8')\n",
        "    return img\n",
        "#Step 3: Define Loss Functions\n",
        "#Define functions to compute the content loss and style loss based on features extracted from intermediate layers of the VGG19 model.\n",
        "\n",
        "#python\n",
        "\n",
        "def get_feature_representations(model, content_path, style_path):\n",
        "    # Load and preprocess content and style images\n",
        "    content_image = preprocess_image(content_path)\n",
        "    style_image = preprocess_image(style_path)\n",
        "\n",
        "    # Get content and style features from VGG19 model\n",
        "    content_outputs = model(content_image)\n",
        "    style_outputs = model(style_image)\n",
        "\n",
        "    # Extract content and style feature representations\n",
        "    content_features = {layer.name: value for layer, value in zip(model.layers[1:], content_outputs)}\n",
        "    style_features = {layer.name: value for layer, value in zip(model.layers[1:], style_outputs)}\n",
        "\n",
        "    return content_features, style_features\n",
        "\n",
        "def compute_content_loss(content_features, target_features):\n",
        "    return tf.reduce_mean(tf.square(content_features['block5_conv2'] - target_features['block5_conv2']))\n",
        "\n",
        "def gram_matrix(input_tensor):\n",
        "    channels = int(input_tensor.shape[-1])\n",
        "    a = tf.reshape(input_tensor, [-1, channels])\n",
        "    n = tf.shape(a)[0]\n",
        "    gram = tf.matmul(a, a, transpose_a=True)\n",
        "    return gram / tf.cast(n, tf.float32)\n",
        "\n",
        "def compute_style_loss(style_features, target_features):\n",
        "    style_loss = 0\n",
        "    for layer in ['block1_conv1', 'block2_conv1', 'block3_conv1', 'block4_conv1', 'block5_conv1']:\n",
        "        style_output = style_features[layer]\n",
        "        target_output = target_features[layer]\n",
        "\n",
        "        style_gram = gram_matrix(style_output)\n",
        "        target_gram = gram_matrix(target_output)\n",
        "\n",
        "        layer_style_loss = tf.reduce_mean(tf.square(style_gram - target_gram))\n",
        "        style_loss += layer_style_loss / (2 * len(style_features))\n",
        "\n",
        "    return style_loss\n",
        "\n",
        "def compute_total_loss(model, loss_weights, init_image, content_features, style_features):\n",
        "    style_weight, content_weight = loss_weights\n",
        "\n",
        "    model_outputs = model(init_image)\n",
        "    content_outputs = model(content_features['input_1'])\n",
        "    style_outputs = model(style_features['input_1'])\n",
        "\n",
        "    content_loss = compute_content_loss(content_outputs, model_outputs)\n",
        "    style_loss = compute_style_loss(style_outputs, model_outputs)\n",
        "\n",
        "    total_loss = content_weight * content_loss + style_weight * style_loss\n",
        "    return total_loss, content_loss, style_loss\n",
        "#Step 4: Define and Optimize the Style Transfer Process\n",
        "#Define a function to perform gradient descent to minimize the total loss (content loss + style loss) between the content and style images.\n",
        "\n",
        "#python code\n",
        "\n",
        "@tf.function()\n",
        "def run_style_transfer(model, content_path, style_path, num_iterations=1000, content_weight=1e3, style_weight=1e-2):\n",
        "    # Initialize images: content image as the base, and style image for the style transfer\n",
        "    content_features, style_features = get_feature_representations(model, content_path, style_path)\n",
        "    init_image = preprocess_image(content_path)\n",
        "    init_image = tf.Variable(init_image, dtype=tf.float32)\n",
        "\n",
        "    # Define optimizer\n",
        "    optimizer = tf.optimizers.Adam(learning_rate=5, beta_1=0.99, epsilon=1e-1)\n",
        "\n",
        "    # Target loss\n",
        "    loss_weights = (content_weight, style_weight)\n",
        "\n",
        "    # Initialize best results\n",
        "    best_cost, best_image = float('inf'), None\n",
        "\n",
        "    # Iterations\n",
        "    let's details.detectChanges"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "bIHm2MogTqyh"
      }
    }
  ]
}