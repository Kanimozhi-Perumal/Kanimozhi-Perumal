# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nmxo7l7jnCiUI2qE81Ih3SFb2P9cnPcw
"""



"""Training a model like GPT-2 (and its successor GPT-3) involves several steps, including dataset preparation, model fine-tuning, and then using the fine-tuned model to generate text based on prompts. Here’s a step-by-step guide on how to approach this:

1. Prepare Your Environment
First, ensure you have a suitable environment for training the model. You can use platforms like Google Colab, AWS EC2, or a local machine with a powerful GPU (recommended for faster training).

2. Obtain the GPT-2 Model
You can use the pre-trained GPT-2 model provided by OpenAI or a Hugging Face Transformers library, which provides an easy-to-use interface for working with GPT-2 and other models.

python
Copy code
# Install transformers library
!pip install transformers
3. Prepare Your Dataset
Your dataset should consist of text that resembles the type of text you want your model to generate. Ensure the dataset is large enough to capture diverse patterns and styles of writing. Clean the dataset to remove unnecessary noise and ensure text quality.

4. Fine-tune GPT-2 on Your Dataset
Fine-tuning involves updating the parameters of the pre-trained GPT-2 model using your custom dataset. This process adapts the model to generate text similar to your training data.

Here’s an example of how you can fine-tune GPT-2 using the Transformers library:

python
Copy code
import torch
from transformers import GPT2LMHeadModel, GPT2Tokenizer, GPT2Config
from transformers import Trainer, TrainingArguments
from transformers import TextDataset, DataCollatorForLanguageModeling

# Load pre-trained GPT-2 model and tokenizer
model_name = 'gpt2'
tokenizer = GPT2Tokenizer.from_pretrained(model_name)
model = GPT2LMHeadModel.from_pretrained(model_name)

# Load and preprocess your custom dataset
# Example: Assuming 'dataset.txt' contains your training data
dataset = TextDataset(
    tokenizer=tokenizer,
    file_path="dataset.txt",  # Path to your dataset file
    block_size=128  # Maximum sequence length
)

# Data collator for language modeling
data_collator = DataCollatorForLanguageModeling(
    tokenizer=tokenizer,
    mlm=False  # MLM is set to False for language modeling
)

# Define training arguments
training_args = TrainingArguments(
    output_dir="./gpt2-finetuned",  # Output directory
    overwrite_output_dir=True,
    num_train_epochs=3,  # Number of training epochs
    per_device_train_batch_size=8,  # Batch size (per device)
    save_steps=10_000,  # Save checkpoint every specified number of steps
    save_total_limit=2,  # Limit the total number of checkpoints saved
)

# Initialize Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    data_collator=data_collator,
    train_dataset=dataset,
)

# Start training
trainer.train()
5. Generate Text
Once the model is trained, you can use it to generate text based on given prompts. Here’s an example of how to generate text using the fine-tuned model:

python
Copy code
# Example prompt
prompt = "Once upon a time"

# Encode prompt
input_ids = tokenizer.encode(prompt, return_tensors='pt')

# Generate text
output = model.generate(
    input_ids,
    max_length=100,  # Maximum length of generated text
    num_return_sequences=1,  # Number of different sequences to generate
    no_repeat_ngram_size=2,  # Avoid repeating sequences of two or more tokens
    top_k=50,  # Generate from top 50 most likely tokens
    top_p=0.95,  # Generate until the total cumulative probability reaches 0.95
    temperature=0.7,  # Higher temperature encourages more diversity
)

# Decode and print generated text
generated_text = tokenizer.decode(output[0], skip_special_tokens=True)
print(generated_text)
6. Refinement and Evaluation
Fine-tuning and generating text is an iterative process. Experiment with different training parameters, dataset sizes, and generation techniques (like adjusting temperature or top-k sampling) to achieve better results. Evaluate the generated text to ensure coherence and relevance to the original dataset.

7. Considerations
Compute Resources: Training GPT-2 can be resource-intensive, particularly for larger datasets and models like GPT-2. Consider using GPUs or cloud services for faster training.

Ethical Considerations: Ensure your dataset and generated text adhere to ethical guidelines, avoiding biases and harmful content.

By following these steps, you can effectively train and use a GPT-2 model to generate coherent and contextually relevant text based on a given prompt, tailored to the style and structure of your custom dataset.



"""

import torch
from transformers import GPT2LMHeadModel, GPT2Tokenizer, GPT2Config
from transformers import Trainer, TrainingArguments
from transformers import TextDataset, DataCollatorForLanguageModeling

# Load pre-trained GPT-2 model and tokenizer
model_name = 'gpt2'
tokenizer = GPT2Tokenizer.from_pretrained(model_name)
model = GPT2LMHeadModel.from_pretrained(model_name)

# Load and preprocess your custom dataset
# Example: Assuming 'dataset.txt' contains your training data
dataset = TextDataset(
    tokenizer=tokenizer,
    file_path="dataset.txt",  # Path to your dataset file
    block_size=128  # Maximum sequence length
)

# Data collator for language modeling
data_collator = DataCollatorForLanguageModeling(
    tokenizer=tokenizer,
    mlm=False  # MLM is set to False for language modeling
)

# Define training arguments
training_args = TrainingArguments(
    output_dir="./gpt2-finetuned",  # Output directory
    overwrite_output_dir=True,
    num_train_epochs=3,  # Number of training epochs
    per_device_train_batch_size=8,  # Batch size (per device)
    save_steps=10_000,  # Save checkpoint every specified number of steps
    save_total_limit=2,  # Limit the total number of checkpoints saved
)

# Initialize Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    data_collator=data_collator,
    train_dataset=dataset,
)

# Start training
trainer.train()
5. Generate Text
Once the model is trained, you can use it to generate text based on given prompts. Here’s an example of how to generate text using the fine-tuned model:

python
Copy code
# Example prompt
prompt = "Once upon a time"

# Encode prompt
input_ids = tokenizer.encode(prompt, return_tensors='pt')

# Generate text
output = model.generate(
    input_ids,
    max_length=100,  # Maximum length of generated text
    num_return_sequences=1,  # Number of different sequences to generate
    no_repeat_ngram_size=2,  # Avoid repeating sequences of two or more tokens
    top_k=50,  # Generate from top 50 most likely tokens
    top_p=0.95,  # Generate until the total cumulative probability reaches 0.95
    temperature=0.7,  # Higher temperature encourages more diversity
)

# Decode and print generated text
generated_text = tokenizer.decode(output[0], skip_special_tokens=True)
print(generated_text)